{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: $<$Rajarshi Basak$>$ \n",
    "\n",
    "**Kaggle Username**: $<$insert username here$>$\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LowerAlphabetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r',\n",
    "                   's','t','u','v','w','x','y','z']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(letters)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(letter) for letter in letters])\n",
    "            \n",
    "        return csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class UpperAlphabetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        letters = ['A','B','C','D','E','F','G','H','I','J',\n",
    "                   'K','L','M','N','O','P','Q','R','S','T','U','V','X','Y','Z']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(letters)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(letter) for letter in letters])\n",
    "            \n",
    "        return csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DigitTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "         \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(numbers)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(number) for number in numbers])\n",
    "            \n",
    "        return csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SpecialCharTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        chars = [\"'\", '\"', '!', \"...\", ',', '?','~']\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(chars)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(char) for char in chars])\n",
    "            \n",
    "        return csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SpaceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import numpy as np \n",
    "        from scipy.sparse import csr_matrix\n",
    "        \n",
    "        spaces = [' ','  ', '   ', '    ']\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples), len(spaces)))\n",
    "        \n",
    "        # Loop over examples and count letters \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([x.count(space) for space in spaces])\n",
    "            \n",
    "        return csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "        \n",
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        return None;\n",
    "        #self.vectorizer = CountVectorizer(stop_words = 'english', ngram_range=(1,4))\n",
    "\n",
    "    def train_test_modifier(self, examples, trope, page):\n",
    "        \n",
    "        self.trope1 = []\n",
    "        self.page1 = []\n",
    "        \n",
    "        for j in trope:\n",
    "            j = re.findall('[A-Z][^A-Z]*', j)\n",
    "            j = \" \".join(j)\n",
    "            self.trope1.append(\" \" + j + \" \")\n",
    "            \n",
    "        for j in page:\n",
    "            #j = re.findall('[A-Z][^A-Z]*', j)\n",
    "            #j = \" \".join(j)\n",
    "            self.page1.append(\" \" + j + \" \")\n",
    "        \n",
    "        newex = dict()\n",
    "        \n",
    "        newex['sentence'] = examples\n",
    "        newex['trope1'] = self.trope1\n",
    "        newex['page1'] = self.page1\n",
    "        \n",
    "        return newex\n",
    "    \n",
    "    def build_train_features(self, examples, trope, page):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        page1 = []\n",
    "        self.page1 = []\n",
    "        page2 = []\n",
    "        self.trope1 = []\n",
    "        trope2 = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Combining features using FeatureUnion\n",
    "        \n",
    "        \n",
    "        self.allmyfeatures = FeatureUnion([ ('sentence_bagOfWords', Pipeline([\n",
    "                                                ('sentence_sel1', ItemSelector(key='sentence')),\n",
    "                                                (\"bag-of-words-sent\", CountVectorizer(stop_words='english', \n",
    "                                                                            ngram_range=(-1,6), lowercase = False))])),\n",
    "                                            ('sentence_tfidf', Pipeline([\n",
    "                                                ('sentence_sel2', ItemSelector(key='sentence')),\n",
    "                                                (\"tf_idf\", TfidfVectorizer(min_df = 0.001, max_df = 0.999, norm = 'l2'))])),\n",
    "                                            ('sentence_loweralpha', Pipeline([\n",
    "                                                ('sentence_sel3', ItemSelector(key='sentence')),\n",
    "                                                (\"lower-alphabets\", LowerAlphabetTransformer())])),\n",
    "                                            #('sentence_spacer', Pipeline([\n",
    "                                            #    ('sentence_sel4', ItemSelector(key='sentence')),\n",
    "                                            #    (\"spaces\", SpaceTransformer())])),\n",
    "                                            #('sentence_specialchar', Pipeline([\n",
    "                                            #    ('sentence_sel5', ItemSelector(key='sentence')),\n",
    "                                            #    (\"specialchar\", SpecialCharTransformer())])),\n",
    "                                            ('trope_OneHotEncoding', Pipeline([\n",
    "                                                ('trope_sel1', ItemSelector(key='trope1')),\n",
    "                                                (\"bag-of-words-trope\", CountVectorizer(stop_words='english',ngram_range=(0,3), lowercase = False))])),\n",
    "                                            #('trope_loweralpha', Pipeline([\n",
    "                                            #    ('trope_sel2', ItemSelector(key='trope1')),\n",
    "                                            #    (\"lower-alphabets-trope\", LowerAlphabetTransformer())])),\n",
    "                                            #('trope_upperalpha', Pipeline([\n",
    "                                            #    ('trope_sel3', ItemSelector(key='trope1')),\n",
    "                                            #    (\"upper-alphabets-trope\", UpperAlphabetTransformer())])),\n",
    "                                            #('page_OneHotEncoding', Pipeline([\n",
    "                                            #    ('page_sel1', ItemSelector(key='page1')),\n",
    "                                            #    (\"bag-of-words-page\", CountVectorizer())]))\n",
    "                                            #('page_tfidf', Pipeline([\n",
    "                                            #    ('page_sel2', ItemSelector(key='page1')),\n",
    "                                            #    (\"tf_idf\", TfidfVectorizer())])),\n",
    "                                            #('page_digits', Pipeline([\n",
    "                                            #    ('page_sel2', ItemSelector(key='page1')),\n",
    "                                            #    (\"digits-page\", DigitTransformer())]))\n",
    "                                          ]);\n",
    "        \n",
    "        newex = dict()\n",
    "        \n",
    "        newex = self.train_test_modifier(examples,trope, page)\n",
    "        \n",
    "        Z1 = self.allmyfeatures.fit_transform(newex)\n",
    "        \n",
    "        #print(\"Z1 has type \", type(Z1))\n",
    "        print(\"The final matrix with all features has shape \", Z1.shape)\n",
    "        \n",
    "        return Z1\n",
    "\n",
    "    def get_test_features(self, examples, page, trope):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        '''A1 = self.allmyfeatures.transform(examples)\n",
    "        A2 = self.allmyfeatures.transform(page)\n",
    "        A3 = self.allmyfeatures.transform(trope)'''\n",
    "        \n",
    "        #Af = np.concatenate((A1, A2, A3), axis=1)\n",
    "        \n",
    "        newex = dict()\n",
    "        \n",
    "        '''newex['sentence'] = examples\n",
    "        newex['trope1'] = self.trope1'''\n",
    "        \n",
    "        newex = self.train_test_modifier(examples,trope, page)\n",
    "        \n",
    "        \n",
    "        return self.allmyfeatures.transform(newex)\n",
    "\n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "                \n",
    "    def train_model(self, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        from sklearn.model_selection import learning_curve\n",
    "        \n",
    "        # load data \n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        dfTrain.head()\n",
    "        \n",
    "        # get training features and labels \n",
    "        self.X_train = self.build_train_features(list(dfTrain[\"sentence\"]), list(dfTrain[\"trope\"]), list(dfTrain[\"page\"]))\n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        \n",
    "        #Do train test split using sklearn package\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X_train, self.y_train, test_size=0.2, random_state=1230)\n",
    "        \n",
    "        print (X_train.shape)\n",
    "        print (y_train.shape)\n",
    "        print (X_test.shape)\n",
    "        print (y_test.shape)\n",
    "        print (\"The type of X_train is \" ,type(X_train))\n",
    "        \n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        #self.logreg.fit(self.X_train, self.y_train)\n",
    "        self.logreg.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "        #Performance on train-test-split on training data\n",
    "        print(\"Accuracy on training data = {:.3f}\".format(self.logreg.score(X_train, y_train)))\n",
    "        print(\"Accuracy on validation data = {:.3f}\".format(self.logreg.score(X_test, y_test)))\n",
    "        \n",
    "\n",
    "        #Performance on cross-validation on training data\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv=5)\n",
    "        print(scores)\n",
    "        print(\"Mean Accuracy in Cross-Validation = {:.3f}\".format(scores.mean()))\n",
    "        \n",
    "        ylim=None\n",
    "        title = \"Learning Curve\"\n",
    "        \n",
    "        \n",
    "        #Plotting the learning curve for our model\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(\"Number of Training examples\")\n",
    "        plt.ylabel(\"Errors\")\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            self.logreg, self.X_train, self.y_train, cv=5, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        plt.plot(train_sizes,1 - train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training error\")\n",
    "        plt.plot(train_sizes,1 - test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation error\")\n",
    "\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        \n",
    "    def error_analysis(self):\n",
    "        \n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        \n",
    "        \n",
    "        # Fit Model to Train Data\n",
    "        limit = .5\n",
    "        test_size = .2\n",
    "        train = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        # Split train data into train and validation data (also shuffles rows)\n",
    "        \n",
    "        train_limited = train.sample(frac=limit)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(train_limited, train_limited['spoiler'], test_size=test_size)\n",
    "        xtr = self.build_train_features(list(X_train[\"sentence\"]),list(X_train[\"trope\"]), list(X_train[\"page\"]))\n",
    "        ytr = np.array(y_train, dtype=int)\n",
    "        newex3 = self.train_test_modifier(list(X_val[\"sentence\"]), list(X_val[\"trope\"]), list(X_val[\"page\"]))\n",
    "        xval = self.allmyfeatures.transform(newex3)\n",
    "        yval = np.array(y_val, dtype=int)\n",
    "        self.logreg = LogisticRegression(random_state=1234)\n",
    "        self.logreg.fit(xtr, ytr)\n",
    "        \n",
    "        pred_val = self.logreg.predict(xval)\n",
    "        print (\"Validation Accuracy: \", accuracy_score(yval, pred_val, 0))\n",
    "        \n",
    "        # Error Analysis\n",
    "        report = classification_report(y_val, pred_val)\n",
    "        print(report)\n",
    "    \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        #dfTest.head()\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(list(dfTest[\"sentence\"]), list(dfTest[\"page\"]), list(dfTest[\"trope\"]))\n",
    "       \n",
    "        print (\"The shape of X_test is \", self.X_test.shape)\n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction-17.csv\", index=True, index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final matrix with all features has shape  (11970, 472182)\n",
      "(9576, 472182)\n",
      "(9576,)\n",
      "(2394, 472182)\n",
      "(2394,)\n",
      "The type of X_train is  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Accuracy on training data = 0.999\n",
      "Accuracy on validation data = 0.713\n",
      "[ 0.67390397  0.68267223  0.64369256  0.65649812  0.65733389]\n",
      "Mean Accuracy in Cross-Validation = 0.663\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(random_state=1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data and produce Kaggle submission file \n",
    "feat.model_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/spoilers/train.csv\", sep = ',', names = ['sentence', 'spoiler', 'page', 'trope'] )\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Solution to Motivation and Analysis:**<br>\n",
    "- For testing the models, a train-validation split of 80-20 was used on the training set, and then a 5-fold cross-validation set was used from the training set to evaluate the mean cross-validation score.\n",
    "\n",
    "- On trying the CountVectorzier (which builds a bag-of-words model of unigrams/words from the vocabulary) with a list of in-built stop-words in English, the accuracies on the train set and validation set were 93.9% and 66.2% respectively, while the mean accuracy in cross-validation was 59.6%.\n",
    "\n",
    "- When n-grams with higher values of n (eg. n = 2 and n = 3) were tried on the CountVectorizer, the accuracies were seen to rise, since bigrams and trigrams capture whether a review is a spoiler or not better that just single words. In addition, a TF-IDF Vectorizer was also used since it gives more weight to terms or words that appear frequently in a particular example or sentence but appear infrequently in many sentences across the entire dataset. A Custom Transformer for Alphabets (Lowercase and Uppercase) and Digits (0-9) similar to the XYZTransformer shown in class was also built, and the output csr matrices from these three feature builders (i.e. CountVectorizer, TF-IDF Vectorizer, and the Alphabet and Digit Transformer) were concatenated using FeatureUnion into a single matrix. Since for n-grams higher than n = 5, there was a considereabe drop in the accuracy, the n-grams used here were n = 1 to n = 5. The accuracies on the train set and validation set were 99.5% and 67.7% respectively, while the mean accuracy in cross-validation was 62.8%\n",
    "\n",
    "- It was observed that the highest accuracy from the CountVectorizer was for n = 1 to n = 4 (since the higher n-grams were leading to a negative correlation as not too many sentences which were marked spoilers had a common set of n-grams with n = 6,7 or 8). Also, the upper-case alphabets turned out to be redundant features, since an Upper-case alphabet appears only at the start of a sentence or for a common or proper noun, which could appear in Spoiler or a Non-spoiler. On the other hand, a lowercase letter is far more likely to be a good predictor (than an uppercase letter) since certain words which are common in Spoilers (eg. kills) have a larger number of a lowercase letter. Similarly, digits were also redundant features as there wasn't a higher or lower concentration of digits in either spoilers or non-spoilers. However, it was observed that many of the sentences that were spoilers seemed to have more than one whitespace between consecutive words in a sentence, and hence 2, 3, and 4 consecutive whitespaces was added as features. For this new feature set, the accuracies on the train set and validation set were 99.3% and 72.0% respectively, while the mean accuracy in cross-validation was 68.6%.\n",
    "\n",
    "- In the next iteration, the tropes field from the dataset was used. Regular expressions was employed to split each of the trope entries to the individual words in them. Finally the CountVectorizer (with unigrams) was used to build a bag-of-words and this time a pipeline was used to pass each of the hand-built features from the previous steps, including the CountVectorizer for the tropes. This time, the accuracies on the train set and validation set were 99.8% and 75.4% respectively, while the mean accuracy in cross-validation was 71.4%. The increase in accuracy proves that tropes is a good predictor for a review, which should be the case since certian tv-shows are likely to have tropes that could naturally have possible spoilers (eg. action/thriller or detective) whereas other tv shows like comedy and reality shows are far less likely to have possible spoilers.\n",
    "\n",
    "- Finally, some of the parameters that were passed in CountVectorizer were fined tuned, and the accuracies on the train set and validation set were 99.8% and 76.6% respectively, while the mean accuracy in cross-validation was 72.0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Analysis\n",
    "feat.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows the classification report of our classifier, which includes the precision, recall, f1-score for the Spoilers (True) and Non-spoilers (False), and mean scores of the same for both the Spoilers and the Non-spoilers combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "***\n",
    "\n",
    "- Don't use all the data until you're ready. \n",
    "\n",
    "- Examine the features that are being used.\n",
    "\n",
    "- Do error analyses.\n",
    "\n",
    "- If you have questions that aren’t answered in this list, feel free to ask them on Piazza.\n",
    "\n",
    "### FAQs \n",
    "***\n",
    "\n",
    "> Can I heavily modify the FeatEngr class? \n",
    "\n",
    "Totally.  This was just a starting point.  The only thing you cannot modify is the LogisticRegression classifier.  \n",
    "\n",
    "> Can I look at TV Tropes?\n",
    "\n",
    "In order to gain insight about the data yes, however, your feature extraction cannot use any additional data (beyond what I've given you) from the TV Tropes webpage.\n",
    "\n",
    "> Can I use IMDB, Wikipedia, or a dictionary?\n",
    "\n",
    "Yes, but you are not required to. So long as your features are fully automated, they can use any dataset other than TV Tropes. Be careful, however, that your dataset does not somehow include TV Tropes (e.g. using all webpages indexed by Google will likely include TV Tropes).\n",
    "\n",
    "> Can I combine features?\n",
    "\n",
    "Yes, and you probably should. This will likely be quite effective.\n",
    "\n",
    "> Can I use Mechanical Turk?\n",
    "\n",
    "That is not fully automatic, so no. You should be able to run your feature extraction without any human intervention. If you want to collect data from Mechanical Turk to train a classifier that you can then use to generate your features, that is fine. (But that’s way too much work for this assignment.)\n",
    "\n",
    "> Can I use a Neural Network to automatically generate derived features? \n",
    "\n",
    "No. This assignment is about your ability to extract meaningful features from the data using your own experimentation and experience.\n",
    "\n",
    "> What sort of improvement is “good” or “enough”?\n",
    "\n",
    "If you have 10-15% improvement over the baseline (on the Public Leaderboard) with your features, that’s more than sufficient. If you fail to get that improvement but have tried reasonable features, that satisfies the requirements of assignment. However, the extra credit for “winning” the class competition depends on the performance of other students.\n",
    "\n",
    "> Where do I start?  \n",
    "\n",
    "It might be a good idea to look at the in-class notebook associated with the Feature Engineering lecture where we did similar experiments. \n",
    "\n",
    "\n",
    "> Can I use late days on this assignment? \n",
    "\n",
    "You can use late days for the write-up submission, but the Kaggle competition closes at **4:59pm on Friday February 23rd**\n",
    "\n",
    "> Why does it say that the competition ends at 11:59pm when the assignment says 4:59pm? \n",
    "\n",
    "The end time/date are in UTC.  11:59pm UTC is equivalent to 4:59pm MST.  Kaggle In-Class does not allow us to change this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
